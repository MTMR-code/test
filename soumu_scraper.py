import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from datetime import datetime, date
import time
import urllib.parse
from dateutil.relativedelta import relativedelta

def generate_date_urls(start_year=2009, end_year=None):
    """
    2009å¹´1æœˆã‹ã‚‰æŒ‡å®šå¹´ã¾ã§ï¼ˆã¾ãŸã¯ç¾åœ¨ã¾ã§ï¼‰ã®URLã‚’ç”Ÿæˆã™ã‚‹
    """
    if end_year is None:
        end_year = datetime.now().year
    
    urls = []
    current_date = date(start_year, 1, 1)
    end_date = date(end_year, 12, 31)
    
    while current_date <= end_date:
        # URLå½¢å¼: YYMMm.html (ä¾‹: 2501m.html = 2025å¹´1æœˆ)
        # ãŸã ã—2023å¹´10æœˆã®ã¿2310.htmlï¼ˆmãªã—ï¼‰
        year_short = str(current_date.year)[2:]  # ä¸‹2æ¡
        month = f"{current_date.month:02d}"
        
        # 2023å¹´10æœˆã®ç‰¹åˆ¥å‡¦ç†
        if current_date.year == 2023 and current_date.month == 10:
            url_suffix = f"{year_short}{month}.html"
        else:
            url_suffix = f"{year_short}{month}m.html"
        
        url = f"https://www.soumu.go.jp/menu_news/s-news/{url_suffix}"
        
        urls.append({
            'url': url,
            'year': current_date.year,
            'month': current_date.month,
            'period': f"{current_date.year}å¹´{current_date.month}æœˆ"
        })
        
        # æ¬¡ã®æœˆã¸
        current_date = current_date + relativedelta(months=1)
    
    return urls

def scrape_soumu_press_releases(url, period_info=None):
    """
    ç·å‹™çœã®å ±é“è³‡æ–™ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹
    """
    if period_info:
        print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {period_info['period']} ({url})")
    else:
        print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
    
    try:
        # ãƒšãƒ¼ã‚¸ã®å–å¾—
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º
        if response.encoding == 'ISO-8859-1':
            # ç·å‹™çœã®ãƒšãƒ¼ã‚¸ã¯Shift_JISã®å¯èƒ½æ€§ãŒé«˜ã„
            response.encoding = 'shift_jis'
        elif response.encoding == 'utf-8':
            # UTF-8ã®å ´åˆã¯ãã®ã¾ã¾
            pass
        else:
            # ãã®ä»–ã®å ´åˆã¯Shift_JISã‚’è©¦ã™
            response.encoding = 'shift_jis'
        
        # HTMLã®è§£æ
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å ±é“è³‡æ–™ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
        press_releases = []
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®è¡Œã‚’å–å¾—
        table = soup.find('table')
        if table:
            rows = table.find_all('tr')
            
            for row in rows[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                cells = row.find_all('td')
                if len(cells) >= 3:
                    # æ—¥ä»˜ã®æŠ½å‡º
                    date_cell = cells[0].get_text(strip=True)
                    
                    # å†…å®¹ã®æŠ½å‡ºï¼ˆãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                    content_cell = cells[1]
                    content_link = content_cell.find('a')
                    if content_link:
                        content = content_link.get_text(strip=True)
                        # ãƒªãƒ³ã‚¯URLã‚‚å–å¾—
                        link_url = content_link.get('href', '')
                        if link_url and not link_url.startswith('http'):
                            link_url = urllib.parse.urljoin(url, link_url)
                    else:
                        content = content_cell.get_text(strip=True)
                        link_url = ''
                    
                    # éƒ¨å±€ã®æŠ½å‡º
                    department = cells[2].get_text(strip=True)
                    
                    # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
                    def clean_text(text):
                        if not text:
                            return text
                        # æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–
                        text = str(text).strip()
                        # ä¸è¦ãªç©ºç™½æ–‡å­—ã‚’é™¤å»
                        text = ' '.join(text.split())
                        return text
                    
                    # ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
                    press_releases.append({
                        'ç™ºè¡¨æ—¥': clean_text(date_cell),
                        'å†…å®¹': clean_text(content),
                        'éƒ¨å±€': clean_text(department),
                        'ãƒªãƒ³ã‚¯URL': clean_text(link_url),
                        'å¯¾è±¡æœŸé–“': clean_text(period_info['period'] if period_info else '')
                    })
        
        print(f"å–å¾—ã—ãŸå ±é“è³‡æ–™æ•°: {len(press_releases)}ä»¶")
        
        # DataFrameã«å¤‰æ›
        df = pd.DataFrame(press_releases)
        
        if not df.empty:
            # æ—¥ä»˜ã®æ­£è¦åŒ–
            df['ç™ºè¡¨æ—¥'] = df['ç™ºè¡¨æ—¥'].apply(normalize_date)
            
            return df
        else:
            print("å ±é“è³‡æ–™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame()
            
    except requests.RequestException as e:
        print(f"HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()
    except Exception as e:
        print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

def scrape_all_months(start_year=2009, end_year=None):
    """
    æŒ‡å®šæœŸé–“ã®å…¨ã¦ã®æœˆã®å ±é“è³‡æ–™ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹
    """
    print(f"=== ç·å‹™çœå ±é“è³‡æ–™ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ===")
    print(f"æœŸé–“: {start_year}å¹´1æœˆ ï½ {end_year or datetime.now().year}å¹´{datetime.now().month}æœˆ")
    print("-" * 60)
    
    # URLãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
    urls = generate_date_urls(start_year, end_year)
    print(f"å¯¾è±¡URLæ•°: {len(urls)}ä»¶")
    
    all_data = []
    successful_count = 0
    failed_count = 0
    
    for i, url_info in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] å‡¦ç†ä¸­...")
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        df = scrape_soumu_press_releases(url_info['url'], url_info)
        
        if not df.empty:
            all_data.append(df)
            successful_count += 1
            print(f"âœ… æˆåŠŸ: {len(df)}ä»¶å–å¾—")
        else:
            failed_count += 1
            print(f"âŒ å¤±æ•—: ãƒ‡ãƒ¼ã‚¿ãªã—")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’ç©ºã‘ã‚‹ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰
        time.sleep(0.1)
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        # ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³æ›¿ãˆï¼ˆæ—¥ä»˜é †ï¼‰
        combined_df = combined_df.sort_values('ç™ºè¡¨æ—¥', ascending=False)
        
        print(f"\n=== ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ ===")
        print(f"æˆåŠŸ: {successful_count}ä»¶")
        print(f"å¤±æ•—: {failed_count}ä»¶")
        print(f"ç·å–å¾—ä»¶æ•°: {len(combined_df)}ä»¶")
        
        return combined_df
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return pd.DataFrame()

def normalize_date(date_str):
    """
    æ—¥ä»˜æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–ã™ã‚‹
    """
    try:
        # æ—¥ä»˜ã®å½¢å¼ã‚’çµ±ä¸€
        if 'å¹´' in date_str and 'æœˆ' in date_str and 'æ—¥' in date_str:
            # æ—¢ã«æ­£ã—ã„å½¢å¼ã®å ´åˆ
            return date_str
        else:
            # ãã®ä»–ã®å½¢å¼ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            return date_str
    except:
        return date_str

def save_to_csv(df, filename):
    """
    DataFrameã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    - æ–‡å­—åŒ–ã‘ã‚’å®Œå…¨ã«é˜²ããŸã‚ã®è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œ
    """
    try:
        # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–å¼·åŒ–ï¼‰
        df_clean = df.copy()
        for col in df_clean.columns:
            if df_clean[col].dtype == 'object':
                # æ–‡å­—åˆ—ã®æ­£è¦åŒ–
                df_clean[col] = df_clean[col].astype(str)
                # æ”¹è¡Œæ–‡å­—ã‚’é™¤å»
                df_clean[col] = df_clean[col].str.replace('\n', ' ').str.replace('\r', ' ')
                # ã‚¿ãƒ–æ–‡å­—ã‚’é™¤å»
                df_clean[col] = df_clean[col].str.replace('\t', ' ')
                # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’å˜ä¸€ã®ç©ºç™½ã«
                df_clean[col] = df_clean[col].str.replace(r'\s+', ' ', regex=True)
                # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
                df_clean[col] = df_clean[col].str.strip()

        # 1. UTF-8 with BOMï¼ˆMac/æœ€æ–°Excelå‘ã‘ï¼‰
        utf8_path = filename
        df_clean.to_csv(utf8_path, index=False, encoding='utf-8-sig')
        print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ(UTF-8 BOM): {utf8_path}")

        # 2. Shift_JISï¼ˆWindows Excelç”¨ - æœ€ã‚‚ç¢ºå®Ÿï¼‰
        if utf8_path.lower().endswith('.csv'):
            shift_jis_path = utf8_path[:-4] + '_shift_jis.csv'
        else:
            shift_jis_path = utf8_path + '_shift_jis.csv'
        
        # Shift_JISã§ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼‰
        df_clean.to_csv(shift_jis_path, index=False, encoding='shift_jis', errors='ignore')
        print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ(Shift_JIS): {shift_jis_path}")

        # 3. CP932ï¼ˆWindows Excelç”¨ï¼‰
        if utf8_path.lower().endswith('.csv'):
            cp932_path = utf8_path[:-4] + '_cp932.csv'
        else:
            cp932_path = utf8_path + '_cp932.csv'
        df_clean.to_csv(cp932_path, index=False, encoding='cp932', errors='ignore')
        print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ(CP932): {cp932_path}")

        # 4. Excelå°‚ç”¨ï¼ˆæ‰‹å‹•ã§BOMä»˜ãShift_JISï¼‰
        if utf8_path.lower().endswith('.csv'):
            excel_path = utf8_path[:-4] + '_excel.csv'
        else:
            excel_path = utf8_path + '_excel.csv'
        
        # æ‰‹å‹•ã§BOMä»˜ãShift_JISã§ä¿å­˜
        with open(excel_path, 'w', encoding='utf-8-sig', newline='') as f:
            # ã¾ãšUTF-8ã§æ›¸ãè¾¼ã¿
            df_clean.to_csv(f, index=False, encoding='utf-8')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§Shift_JISã§å†ä¿å­˜
        with open(excel_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        with open(excel_path, 'w', encoding='shift_jis', newline='') as f:
            f.write(content)
        
        print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ(Excelå°‚ç”¨): {excel_path}")

        # 5. ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç¢ºå®Ÿã«èª­ã‚ã‚‹å½¢å¼ï¼‰
        if utf8_path.lower().endswith('.csv'):
            txt_path = utf8_path[:-4] + '_text.txt'
        else:
            txt_path = utf8_path + '_text.txt'
        
        with open(txt_path, 'w', encoding='utf-8', newline='') as f:
            f.write("ç™ºè¡¨æ—¥\tå†…å®¹\téƒ¨å±€\tãƒªãƒ³ã‚¯URL\tå¯¾è±¡æœŸé–“\n")
            for _, row in df_clean.iterrows():
                f.write(f"{row['ç™ºè¡¨æ—¥']}\t{row['å†…å®¹']}\t{row['éƒ¨å±€']}\t{row['ãƒªãƒ³ã‚¯URL']}\t{row['å¯¾è±¡æœŸé–“']}\n")
        print(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ(TSVå½¢å¼): {txt_path}")

        print("\nğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–æ¸ˆã¿ï¼‰:")
        print(f"  - {excel_path} (Excelå°‚ç”¨ - æœ€ã‚‚æ¨å¥¨)")
        print(f"  - {shift_jis_path} (Shift_JIS - Windows Excelç”¨)")
        print(f"  - {cp932_path} (CP932 - Windows Excelç”¨)")
        print(f"  - {utf8_path} (UTF-8 BOM - Mac/Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”¨)")
        print(f"  - {txt_path} (ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ - ç¢ºå®Ÿã«èª­ã‚ã‚‹)")

        return True
    except Exception as e:
        print(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("=== ç·å‹™çœå ±é“è³‡æ–™ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ===")
    print("2009å¹´1æœˆã‹ã‚‰ç¾åœ¨ã¾ã§å…¨ã¦ã®æœˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™")
    print("-" * 60)
    
    # ä¸€æ‹¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    df = scrape_all_months(start_year=2009)
    
    if not df.empty:
        # çµæœã®è¡¨ç¤º
        print("\n=== è©³ç´°çµ±è¨ˆ ===")
        print(f"ç·å–å¾—ä»¶æ•°: {len(df)}ä»¶")
        
        print("\néƒ¨å±€åˆ¥é›†è¨ˆ:")
        dept_counts = df['éƒ¨å±€'].value_counts()
        for dept, count in dept_counts.head(10).items():  # ä¸Šä½10ä»¶ã®ã¿è¡¨ç¤º
            print(f"  {dept}: {count}ä»¶")
        
        print("\nå¹´åˆ¥é›†è¨ˆ:")
        df['å¹´'] = df['ç™ºè¡¨æ—¥'].str.extract(r'(\d{4})å¹´')
        year_counts = df['å¹´'].value_counts().sort_index()
        for year, count in year_counts.items():
            print(f"  {year}å¹´: {count}ä»¶")
        
        print("\næœˆåˆ¥é›†è¨ˆï¼ˆç›´è¿‘12ãƒ¶æœˆï¼‰:")
        df['æœˆ'] = df['ç™ºè¡¨æ—¥'].str.extract(r'(\d{1,2})æœˆ')
        recent_data = df.head(100)  # ç›´è¿‘100ä»¶ã§æœˆåˆ¥é›†è¨ˆ
        month_counts = recent_data['æœˆ'].value_counts().sort_index()
        for month, count in month_counts.items():
            print(f"  {month}æœˆ: {count}ä»¶")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"soumu_press_releases_all_{timestamp}.csv"
        
        if save_to_csv(df, filename):
            print(f"\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")
            print(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")
            print(f"ğŸ“… æœŸé–“: 2009å¹´1æœˆ ï½ ç¾åœ¨")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            print("\n=== ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€æ–°5ä»¶ï¼‰===")
            print(df.head().to_string(index=False))
        else:
            print("âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
    else:
        print("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ")

def main_single_month():
    """
    å˜ä¸€æœˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå¾“æ¥ã®æ©Ÿèƒ½ï¼‰
    """
    url = "https://www.soumu.go.jp/menu_news/s-news/2501m.html"
    
    print("=== ç·å‹™çœå ±é“è³‡æ–™ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå˜ä¸€æœˆï¼‰ ===")
    print(f"å¯¾è±¡URL: {url}")
    print("-" * 50)
    
    df = scrape_soumu_press_releases(url)
    
    if not df.empty:
        print(f"\nå–å¾—ä»¶æ•°: {len(df)}ä»¶")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"soumu_press_releases_single_{timestamp}.csv"
        
        if save_to_csv(df, filename):
            print(f"âœ… ä¿å­˜å®Œäº†: {filename}")
        else:
            print("âŒ ä¿å­˜å¤±æ•—")
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")

if __name__ == "__main__":
    main()
